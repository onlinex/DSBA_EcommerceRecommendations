{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:48:25) \n",
      "[Clang 14.0.6 ]\n",
      "Spark version: 3.3.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pyspark\n",
    "from pyspark.ml.recommendation import ALS\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, FloatType, IntegerType, LongType\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.datasets import movielens\n",
    "from recommenders.utils.notebook_utils import is_jupyter\n",
    "from recommenders.datasets.spark_splitters import spark_random_split\n",
    "from recommenders.evaluation.spark_evaluation import SparkRatingEvaluation, SparkRankingEvaluation\n",
    "from recommenders.utils.spark_utils import start_or_get_spark\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Spark version: {}\".format(pyspark.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top k items to recommend\n",
    "TOP_K = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Set up spark context</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/18 22:24:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# the following settings work well for debugging locally on VM - change when running on a cluster\n",
    "# set up a giant single executor with many threads and specify memory cap\n",
    "spark = start_or_get_spark(\"ALS PySpark\", memory=\"10g\")\n",
    "spark.conf.set(\"spark.sql.analyzer.failAmbiguousSelfJoin\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setCheckpointDir('./checkpoints')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Load the MovieLens dataset</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names for the dataset\n",
    "COL_USER = \"UserId\"\n",
    "COL_ITEM = \"ItemId\"\n",
    "COL_RATING = \"Rating\"\n",
    "COL_TIMESTAMP = \"Timestamp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The DataFrame-based API for ALS currently only supports integers for user and item ids.\n",
    "schema = StructType(\n",
    "    (\n",
    "        StructField(COL_USER, IntegerType()),\n",
    "        StructField(COL_ITEM, IntegerType()),\n",
    "        StructField(COL_RATING, FloatType()),\n",
    "        StructField(COL_TIMESTAMP, LongType()),\n",
    "    )\n",
    ")\n",
    "\n",
    "train = spark.read.load('../data/sas/train.csv', format=\"csv\", header=\"true\", sep=',', schema=schema)\n",
    "test = spark.read.load('../data/sas/test.csv', format=\"csv\", header=\"true\", sep=',', schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N train 67791\n",
      "N test 22487\n"
     ]
    }
   ],
   "source": [
    "print (\"N train\", train.cache().count())\n",
    "print (\"N test\", test.cache().count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Train ALS</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {\n",
    "    \"userCol\": COL_USER,\n",
    "    \"itemCol\": COL_ITEM,\n",
    "    \"ratingCol\": COL_RATING,\n",
    "}\n",
    "\n",
    "als = ALS(\n",
    "    rank=10,\n",
    "    maxIter=100,\n",
    "    implicitPrefs=False,\n",
    "    regParam=0.05,\n",
    "    coldStartStrategy='drop',\n",
    "    nonnegative=False,\n",
    "    seed=42,\n",
    "    **header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 8.455119125000003 seconds for training.\n"
     ]
    }
   ],
   "source": [
    "with Timer() as train_time:\n",
    "    model = als.fit(train)\n",
    "\n",
    "print(\"Took {} seconds for training.\".format(train_time.interval))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the movie recommendation use case, recommending movies that have been rated by the users do not make sense. (because they have already seen them)\n",
    "\n",
    "- Therefore, the rated movies are removed from the recommended items.\n",
    "\n",
    "- In order to achieve this, we recommend all movies to all users, and then remove the user-movie pairs that exist in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/18 22:25:06 WARN Column: Constructing trivially true equals predicate, 'UserId#0 = UserId#0'. Perhaps you need to use aliases.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 689:==============================================>      (174 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 16.840148750000004 seconds for prediction.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "with Timer() as test_time:\n",
    "\n",
    "    # Get the cross join of all user-item pairs and score them.\n",
    "    users = train.select(COL_USER).distinct()\n",
    "    items = train.select(COL_ITEM).distinct()\n",
    "    user_item = users.crossJoin(items)\n",
    "    dfs_pred = model.transform(user_item)\n",
    "\n",
    "    # Remove seen items.\n",
    "    dfs_pred_exclude_train = dfs_pred.alias(\"pred\").join(\n",
    "        train.alias(\"train\"),\n",
    "        (dfs_pred[COL_USER] == train[COL_USER]) & (dfs_pred[COL_ITEM] == train[COL_ITEM]),\n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "    # train.{col_rating} will be null on the previous step, if this pair was not seen before. These are the entries of interest to us.\n",
    "    top_all = dfs_pred_exclude_train.filter(dfs_pred_exclude_train[f\"train.{COL_RATING}\"].isNull()) \\\n",
    "        .select('pred.' + COL_USER, 'pred.' + COL_ITEM, 'pred.' + \"prediction\")\n",
    "\n",
    "    # In Spark, transformations are lazy evaluation\n",
    "    # Use an action to force execute and measure the test time \n",
    "    top_all.cache().count()\n",
    "\n",
    "print(\"Took {} seconds for prediction.\".format(test_time.interval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+\n",
      "|UserId|ItemId|prediction|\n",
      "+------+------+----------+\n",
      "|     0|   190|0.91622233|\n",
      "|     0|   632|0.90225005|\n",
      "|     0|   716| 1.0188911|\n",
      "|     0|   915| 1.1059035|\n",
      "|     0|  1218| 1.0256288|\n",
      "|     0|  1237| 1.2798171|\n",
      "|     0|  1265|0.96745914|\n",
      "|     0|  1327| 1.6871678|\n",
      "|     0|  1478| 0.9677968|\n",
      "|     0|  1578|0.99901414|\n",
      "|     0|  1761|0.98501194|\n",
      "|     0|  1790| 1.2123132|\n",
      "|     0|  1866|0.93400866|\n",
      "|     0|  2248| 1.0757623|\n",
      "|     1|   587| 3.0049047|\n",
      "|     1|   869| 1.0433747|\n",
      "|     1|  1208| 1.8330684|\n",
      "|     1|  1348|  4.342155|\n",
      "|     1|  1357| 1.3046756|\n",
      "|     1|  1677| 1.5294855|\n",
      "+------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_all.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Evaluate how well ALS performs</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Needs all the predictions, to calculate Top-K, NDCG, etc.\n",
    "rank_eval = SparkRankingEvaluation(test, top_all, k = TOP_K, col_user=COL_USER, col_item=COL_ITEM, \n",
    "                                    col_rating=COL_RATING, col_prediction=\"prediction\", \n",
    "                                    relevancy_method=\"top_k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1412:===============================>                        (5 + 4) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\tALS\n",
      "Top K:\t100\n",
      "MAP:\t0.003648\n",
      "NDCG:\t0.018429\n",
      "Precision@K:\t0.002615\n",
      "Recall@K:\t0.049072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Model:\\tALS\",\n",
    "      \"Top K:\\t%d\" % rank_eval.k,\n",
    "      \"MAP:\\t%f\" % rank_eval.map_at_k(),\n",
    "      \"NDCG:\\t%f\" % rank_eval.ndcg_at_k(),\n",
    "      \"Precision@K:\\t%f\" % rank_eval.precision_at_k(),\n",
    "      \"Recall@K:\\t%f\" % rank_eval.recall_at_k(), sep='\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Evaluate rating prediction</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1470:=======================================>            (152 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+----------+----------+\n",
      "|UserId|ItemId|Rating| Timestamp|prediction|\n",
      "+------+------+------+----------+----------+\n",
      "|  2580|   148|   1.0|1497895920|  1.206405|\n",
      "|  2572|   148|   1.0|1497838440| 1.2108516|\n",
      "|  5490|   148|   1.0|1494433680| 1.0447582|\n",
      "|  3287|   148|   1.0|1495538100| 0.7834258|\n",
      "|  3587|   148|   3.0|1498161420|  3.643561|\n",
      "|  4526|   148|   1.0|1496456760|0.99113685|\n",
      "|   122|   148|   1.0|1497617580| 0.9657242|\n",
      "|  4280|   148|   1.0|1496850360| 1.3617122|\n",
      "|  4839|   148|   2.0|1498768860| 2.1397343|\n",
      "|  2824|   148|   1.0|1495063440|  1.037982|\n",
      "|  2150|   148|   2.0|1497262380| 2.3734658|\n",
      "|  1901|   148|   6.0|1496875560|  8.262979|\n",
      "|  2019|   148|   1.0|1497797160|0.92047983|\n",
      "|  5470|   148|   4.0|1498070340| 1.9968414|\n",
      "|  1720|   148|   2.0|1496330760|0.95702773|\n",
      "|  2598|   148|   1.0|1496922600| 1.3155339|\n",
      "|   900|   148|   6.0|1497556140|  5.480857|\n",
      "|   794|   148|   1.0|1498646520| 4.5575533|\n",
      "|  3202|   148|   1.0|1498859820| 1.0086267|\n",
      "|  4156|   148|   1.0|1496842320| 1.8854768|\n",
      "+------+------+------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Generate predicted ratings.\n",
    "prediction = model.transform(test)\n",
    "prediction.cache().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\tALS rating prediction\n",
      "RMSE:\t1.765943\n",
      "MAE:\t0.728060\n",
      "Explained variance:\t0.516858\n",
      "R squared:\t0.507510\n"
     ]
    }
   ],
   "source": [
    "rating_eval = SparkRatingEvaluation(test, prediction, col_user=COL_USER, col_item=COL_ITEM, \n",
    "                                    col_rating=COL_RATING, col_prediction=\"prediction\")\n",
    "\n",
    "print(\"Model:\\tALS rating prediction\",\n",
    "      \"RMSE:\\t%f\" % rating_eval.rmse(),\n",
    "      \"MAE:\\t%f\" % rating_eval.mae(),\n",
    "      \"Explained variance:\\t%f\" % rating_eval.exp_var(),\n",
    "      \"R squared:\\t%f\" % rating_eval.rsquared(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_jupyter():\n",
    "    # Record results with papermill for tests\n",
    "    import scrapbook as sb\n",
    "    sb.glue(\"map\", rank_eval.map_at_k())\n",
    "    sb.glue(\"ndcg\", rank_eval.ndcg_at_k())\n",
    "    sb.glue(\"precision\", rank_eval.precision_at_k())\n",
    "    sb.glue(\"recall\", rank_eval.recall_at_k())\n",
    "    sb.glue(\"rmse\", rating_eval.rmse())\n",
    "    sb.glue(\"mae\", rating_eval.mae())\n",
    "    sb.glue(\"exp_var\", rating_eval.exp_var())\n",
    "    sb.glue(\"rsquared\", rating_eval.rsquared())\n",
    "    sb.glue(\"train_time\", train_time.interval)\n",
    "    sb.glue(\"test_time\", test_time.interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup spark instance\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78bff6571efa098a29603b0b132d0489520f19bae7ac06bdfd504caef114317d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
